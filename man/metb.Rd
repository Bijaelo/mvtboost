% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metb.R
\name{metb}
\alias{metb}
\alias{metb.fit}
\title{Boosted decision trees with random effects}
\usage{
metb(y, X, id, n.trees = 5, interaction.depth = 3, n.minobsinnode = 20,
  shrinkage = 0.01, bag.fraction = 0.5, train.fraction = NULL,
  cv.folds = 1, subset = NULL, indep = TRUE, save.mods = FALSE,
  mc.cores = 1, num_threads = 1, verbose = TRUE, ...)

metb.fit(y, X, id, n.trees = 5, interaction.depth = 3,
  n.minobsinnode = 20, shrinkage = 0.01, bag.fraction = 0.5,
  train.fraction = NULL, subset = NULL, indep = TRUE, num_threads = 1,
  save.mods = FALSE, verbose = TRUE, ...)
}
\arguments{
\item{y}{outcome vector (continuous)}

\item{X}{matrix or data frame of predictors}

\item{id}{name or index of grouping variable}

\item{n.trees}{the total number of trees to fit (iterations).}

\item{interaction.depth}{The maximum depth of trees. 1 implies a single split
(stump), 2 implies a tree with 2 splits, etc.}

\item{n.minobsinnode}{minimum number of observations in the terminal nodes
of each tree}

\item{shrinkage}{a shrinkage parameter applied to each tree. Also known as the 
learning rate or step-size reduction.}

\item{bag.fraction}{the fraction of the training set observations randomly 
selected to propose the next tree. This introduces randomnesses into the model 
fit. If \code{bag.fraction<1} then running the same model twice will result in
 similar but different fits.  Using \code{set.seed} ensures reproducibility.}

\item{train.fraction}{of sample used for training}

\item{cv.folds}{number of cross-validation folds. In addition to the usual fit,
will perform cross-validation over a grid of meta-parameters (see details).}

\item{subset}{index of observations to use for training}

\item{indep}{whether random effects are independent or allowed to covary
(default is TRUE, for speed)}

\item{save.mods}{whether the \code{lmer} models fit at each iteration are saved
(required to use \code{predict})}

\item{mc.cores}{number of parallel cores}

\item{num_threads}{number of threads}

\item{verbose}{In the final model fit, will print every `10` trees/iterations.}

\item{...}{arguments passed to gbm.fit}
}
\value{
An \code{metb} object consisting of the following list elements:
\describe{
  \item{\code{yhat}}{Vector of predictions at the best iteration (\code{fixed} + \code{ranef})}
  \item{\code{ranef}}{Vector of random effects at the best iteration}
  \item{\code{fixed}}{Vector of fixed effect predictions at the best iteration}
  \item{\code{shrinkange}}{Amount of shrinkage}
  \item{\code{subset}}{Vector of observations used for training}
  \item{\code{best.trees}}{Best number of trees by training, test, oob, and cv error}
  \item{\code{best.params}}{The best set of meta-parameter values given by CV}
  \item{\code{params}}{A data frame of all meta-parameter combinations and the corresponding CV error}
  \item{\code{sigma}}{The variance due to the grouping variable at each iteration}
  \item{\code{xnames}}{Column names of \code{X}}
  \item{\code{mods}}{List of \code{lmer} models (if \code{save.mods=TRUE})}
  \item{\code{id}}{name or index of the grouping variable}
  \item{\code{trees}}{List of trees fit at each iteration}
  \item{\code{init}}{initial prediction}
  \item{\code{var.type}}{Type of variables (\code{gbm.fit})}
  \item{\code{c.split}}{List of categorical splits (\code{gbm.fit})}
  \item{\code{train.err}}{Training error at each iteration}
  \item{\code{oob.err}}{Out of bag error at each iteration}
  \item{\code{test.err}}{Test error at each iteration}
  \item{\code{cv.err}}{Cross-validation error at each iteration}
}
}
\description{
At each iteration, a single decision tree is fit using \code{gbm.fit}, and 
the terminal node means are allowed to vary by group using \code{lmer}.
}
\details{
Meta-parameter tuning is handled by passing vectors of possible values for 
\code{n.trees}, \code{shrinkage}, \code{indep}, \code{interaction.depth}, 
and \code{n.minobsinnode} and setting \code{cv.folds > 1}. Setting 
\code{mc.cores > 1} will carry out the tuning in parallel by forking via 
\code{mclapply}. Tuning is only done within the training set.

Prediction is most easily carried out by passing the entire \code{X} matrix to
\code{metb}, and specifying the training set using \code{subset}. Otherwise, 
set \code{save.mods=TRUE} and use \code{predict}.
}
\section{Functions}{
\itemize{
\item \code{metb.fit}: Fitting function for \code{metb}
}}

