% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pcboost.R
\name{pcb}
\alias{pcb}
\title{boost principal components of outcomes}
\usage{
pcb(Y, X, n.trees = 100, shrinkage = 0.01, interaction.depth = 1,
  distribution = "gaussian", train.fraction = 1, bag.fraction = 1,
  cv.folds = 1, keep.data = FALSE, s = NULL, compress = FALSE,
  save.cv = FALSE, iter.details = TRUE, verbose = FALSE, mc.cores = 1,
  ...)
}
\arguments{
\item{Y}{vector, matrix, or data.frame for outcome variables with no missing values. To easily compare influences across outcomes and for numerical stability, outcome variables should be scaled to have unit variance.}

\item{X}{vector, matrix, or data.frame of predictors. For best performance, continuous predictors should be scaled to have unit variance. Categorical variables should converted to factors.}

\item{n.trees}{maximum number of trees to be included in the model. Each individual tree is grown until a minimum number observations in each node is reached.}

\item{shrinkage}{a constant multiplier for the predictions from each tree to ensure a slow learning rate. Default is .01. Small shrinkage values may require a large number of trees to provide adequate fit.}

\item{interaction.depth}{fixed depth of trees to be included in the model. A tree depth of 1 corresponds to fitting stumps (main effects only), higher tree depths capture higher order interactions (e.g. 2 implies a model with up to 2-way interactions)}

\item{distribution}{Character vector specifying the distribution of all outcomes. Default is "gaussian" see ?gbm for further details.}

\item{train.fraction}{proportion of the sample used for training the multivariate additive model. If both \code{cv.folds} and \code{train.fraction} are specified, the CV is carried out within the training set.}

\item{bag.fraction}{proportion of the training sample used to fit univariate trees for each response at each iteration. Default: 1}

\item{cv.folds}{number of cross validation folds. Default: 1. Runs k + 1 models, where the k models are run in parallel and the final model is run on the entire sample. If larger than 1, the number of trees that minimize the multivariate MSE averaged over k-folds is reported in \code{object$best.trees}}

\item{keep.data}{a logical variable indicating whether to keep the data stored with the object.}

\item{s}{vector of indices denoting observations to be used for the training sample. If \code{s} is given, \code{train.fraction} is ignored.}

\item{compress}{\code{TRUE/FALSE}. Compress output results list using bzip2 (approx 10\% of original size). Default is \code{FALSE}.}

\item{save.cv}{\code{TRUE/FALSE}. Save all k-fold cross-validation models. Default is \code{FALSE}.}

\item{iter.details}{\code{TRUE/FALSE}. Return training, test, and cross-validation error at each iteration. Default is \code{FALSE}.}

\item{verbose}{If \code{TRUE}, will print out progress and performance indicators for each model.  Default is \code{FALSE}.}

\item{mc.cores}{Number of cores for cross validation.}

\item{...}{additional arguments passed to \code{gbm}. These include \code{distribution}, \code{weights}, \code{var.monotone}, \code{n.minobsinnode}, \code{keep.data}, \code{verbose}, \code{class.stratify.cv}.  Note that other \code{distribution} arguments have not been tested.}
}
\description{
boost principal components of outcomes
}

